import os


def warn(*args, **kwargs):
    pass

import warnings
warnings.warn = warn

# IBM WatsonX impor
from ibm_watsonx_ai.foundation_models import Model
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes

from langchain_ibm import WatsonxLLM
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from langchain_core.messages import HumanMessage, SystemMessage
from langchain.chains import LLMChain  # Still using this for backward compatibility
from dotenv import load_dotenv


def llm_model(prompt_txt, params=None):
    """
    This function utilizes the Granite LLM model from WatsonxLLM to process a given
    prompt and generate a response based on the specified parameters.

    :param prompt_txt: The input text prompt for the LLM to process.
    :type prompt_txt: str
    :param params: Optional custom parameters to configure the LLM's behavior.
                   Defaults will be applied when not specified.
    :type params: dict, optional
    :return: The response generated by the WatsonxLLM based on the input prompt
             and parameters.
    :rtype: str
    """

    model_id = "ibm/granite-3-3-8b-instruct"

    default_params = {
        "max_new_tokens": 256,
        "min_new_tokens": 0,
        "temperature": 0.5,
        "top_p": 0.2,
        "top_k": 1
    }

    if params:
        default_params.update(params)

    # Set up credentials for WatsonxLLM
    url = "https://eu-de.ml.cloud.ibm.com"
    api_key = os.getenv("api_key")
    project_id = os.getenv("project_id")  #"skills-network"
    print(f"url: {url}\napi_key: {api_key}\nproject_id: {project_id}\n")

    credentials = {
        "url": url,
        "api_key": api_key
    }

    # Create LLM directly
    granite_llm = WatsonxLLM(
        model_id=model_id,
        # credentials=credentials,
        url=url,
        apikey=api_key,
        project_id=project_id,
        params=default_params
    )

    response = granite_llm.invoke(prompt_txt)
    return response


# run the function
if __name__ == "__main__":
    load_dotenv()

    params = {
        "max_new_tokens": 128,
        "min_new_tokens": 10,
        "temperature": 0.5,
        "top_p": 0.2,
        "top_k": 1
    }

    prompt = "The wind is "

    # Getting a reponse from the model with the provided prompt and new parameters
    response = llm_model(prompt, params)
    print(f"prompt: {prompt}\n")
    print(f"response : {response}\n")